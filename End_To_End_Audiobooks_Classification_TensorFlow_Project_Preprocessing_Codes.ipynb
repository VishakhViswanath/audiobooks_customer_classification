{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472d1383",
   "metadata": {},
   "source": [
    "# Audiobooks\n",
    "\n",
    "The data is from an Audiobook app regarding the purchase of Audio versions of books. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
    "\n",
    "The idea is that if a customer is of low potenitial, there is no reason to spend any money on advertizing to him/her. \n",
    "If we can focus our efforts only potential customers that are likely to convert again, we can make great savings. \n",
    "Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\n",
    "\n",
    "We have a .csv summarizing the data. \n",
    "There are several variables: Customer ID, Book length in mins_avg (average of all purchases), Book length in minutes_sum (sum of all purchases), Price Paid_avg (average of all purchases), Price paid_sum (sum of all purchases), Review (a Boolean variable), Review (out of 10), Total minutes listened, Completion (from 0 to 1)(Targets), Support requests (number), and Last visited minus purchase date (in days).\n",
    "\n",
    "So these are the inputs (excluding customer ID, as it is completely arbitrary. It's more like a name, than a number).\n",
    "\n",
    "The targets are a Boolean variable (so 0, or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets. So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months or not. 6 months sounds like a reasonable time. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobook way of digesting information.\n",
    "\n",
    "The task is to create a machine learning algorithm, which can predict if a customer will buy audiobooks from the company again or not.\n",
    "\n",
    "We will use TensorFlow to buld a classification model for this problem \n",
    "\n",
    "In this case we have a cleaned Audiobooks library dataset with no missing or null values. We will perform a supervised learning with TensorFlow on this example. This note book consists of standard EDA, converting and saving regular text data as tensors inorder to perform deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01128bd",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26faef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c49de58",
   "metadata": {},
   "source": [
    "# 2. Extract data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f11185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = np.loadtxt(\"Audiobooks_data.csv\", delimiter= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1782f004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9400e+02, 1.6200e+03, 1.6200e+03, ..., 5.0000e+00, 9.2000e+01,\n",
       "        0.0000e+00],\n",
       "       [1.1430e+03, 2.1600e+03, 2.1600e+03, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [2.0590e+03, 2.1600e+03, 2.1600e+03, ..., 0.0000e+00, 3.8800e+02,\n",
       "        0.0000e+00],\n",
       "       ...,\n",
       "       [3.1134e+04, 2.1600e+03, 2.1600e+03, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [3.2832e+04, 1.6200e+03, 1.6200e+03, ..., 0.0000e+00, 9.0000e+01,\n",
       "        0.0000e+00],\n",
       "       [2.5100e+02, 1.6740e+03, 3.3480e+03, ..., 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53d468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e18263",
   "metadata": {},
   "source": [
    "# 3. Balance the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c03953",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "zero_targets_counter = 0\n",
    "\n",
    "indices_to_remove =[]\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bcfba",
   "metadata": {},
   "source": [
    "# 4. Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f81c6bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f20ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71b7c006",
   "metadata": {},
   "source": [
    "## Shuffle the data\n",
    "   Shuffling is done inorder to ensure the randomity of data in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ae0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2103b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80fdcab0",
   "metadata": {},
   "source": [
    "# 5.  Split- Train, Test & Validation\n",
    "   Splittng the data into train, test and valiadtio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44fb0f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Manually splitting data between Train, Test & Validation\n",
    "train_samples_count = int(0.8* samples_count)\n",
    "validation_samples_count = int(0.1* samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe273d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1793.0 3579 0.5009779267951942\n",
      "224.0 447 0.5011185682326622\n",
      "220.0 448 0.49107142857142855\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(train_targets),train_samples_count, np.sum(train_targets)/train_samples_count)\n",
    "print(np.sum(validation_targets),validation_samples_count, np.sum(validation_targets)/validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets)/ test_samples_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614bd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb517cc6",
   "metadata": {},
   "source": [
    "# 6. Saving the preprocesses data as .npz\n",
    "The preprocessed data is now being saved as tensors so that bwe can build the model and perform analysis on the data in the next section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7596a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"Audiobooks_data_train\",inputs=train_inputs,targets=train_targets)\n",
    "np.savez(\"Audiobooks_data_validation\",inputs=validation_inputs,targets=validation_targets)\n",
    "np.savez(\"Audiobooks_data_test\",inputs=test_inputs,targets=test_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207197f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
